{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders.wikipedia import WikipediaLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"mxbai-embed-large\"\n",
    "LLM_MODEL = \"gemma:2b\"\n",
    "\n",
    "def database_uri():\n",
    "    user = os.getenv(\"POSTGRES_USER\", \"postgres\")\n",
    "    password = os.getenv(\"POSTGRES_PASSWORD\", \"\")\n",
    "    server = os.getenv(\"POSTGRES_SERVER\", \"localhost\")\n",
    "    port = os.getenv(\"POSTGRES_PORT\", \"5432\")\n",
    "    db = os.getenv(\"POSTGRES_DB\", \"localrag\")\n",
    "    return f\"postgresql+psycopg2://{user}:{password}@{server}:{port}/{db}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luis/.local/share/virtualenvs/local-rag-lip6Jf8R/lib/python3.12/site-packages/langchain_community/vectorstores/pgvector.py:293: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata.Please note that filtering operators have been changed when using JSOB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create adb migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
    "vector_db = PGVector(\n",
    "    embedding_function=embeddings,\n",
    "    connection_string=database_uri(),\n",
    "    pre_delete_collection=True,\n",
    ")\n",
    "retriever = vector_db.as_retriever()\n",
    "llm = Ollama(model=LLM_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_query(query: str):\n",
    "    loader = WikipediaLoader(query=query, load_max_docs=3)\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=80)\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "    vector_db.add_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_prompt = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\\n\n",
    "Context: {context} \\n\n",
    "Question: {question} \\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", human_prompt)])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A transformer model is a type of neural network architecture that uses the principles of transformer architecture to process sequential data. It is a powerful tool for tasks such as language translation, text summarization, and speech recognition.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is a transformer model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The provided context does not contain information regarding the year of the Transformer paper's publication, so I cannot answer this question from the provided context.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"In what year was the Transformer paper published?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Transformer paper was published in 2017.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_query(\"Transformer Architecture\")\n",
    "rag_chain.invoke(\"In what year was the Transformer paper published?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided text does not contain information regarding langchain, so I cannot answer this question.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). It is an open-source project that integrates with various cloud storage systems, APIs, and web services to facilitate the creation of applications using LLMs.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_query(\"Langchain\")\n",
    "rag_chain.invoke(\"What is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-rag-lip6Jf8R",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
